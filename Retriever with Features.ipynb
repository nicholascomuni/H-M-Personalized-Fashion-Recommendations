{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Personalized Fashion Recommendations\n",
    "\n",
    "# 1. The Overview\n",
    "The objective of this notebook is to setup a recommender model with <a href=\"https://www.tensorflow.org/recommenders?hl=pt-br\">Tensor Flow Recommenders</a> and submit the results to the H&M Personalized Fashion Recommendations Competition on Kaggle.\n",
    "\n",
    "You can find the complete overview of the competition and the datasets by clicking <a href='https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations'>HERE</a>.\n",
    "\n",
    "<b>Lets make a short recap of the H&M Personalized Fashion Recommendations Competition.</b>\n",
    "\n",
    "H&M Group is a family of brands and businesses with 53 online markets and approximately 4,850 stores. Their online store offers shoppers an extensive selection of products to browse through. But with too many choices, customers might not quickly find what interests them or what they are looking for, and ultimately, they might not make a purchase. To enhance the shopping experience, product recommendations are key. More importantly, helping customers make the right choices also has a positive implications for sustainability, as it reduces returns, and thereby minimizes emissions from transportation.\n",
    "\n",
    "In this competition, H&M Group invites competitors to develop product recommendations based on data from previous transactions, as well as from customer and product meta data. The available meta data spans from simple data, such as garment type and customer age, to text data from product descriptions, to image data from garment images.\n",
    "\n",
    "The challenge is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends.\n",
    "\n",
    "# 2. TensorFlow Recommenders\n",
    "\n",
    "TensorFlow Recommenders (TFRS) is a library for building recommender system models.\n",
    "It helps with the full workflow of building a recommender system: data preparation, model formulation, training, evaluation, and deployment.\n",
    "It's built on Keras and aims to have a gentle learning curve while still giving you the flexibility to build complex models.\n",
    "\n",
    "TFRS makes it possible to:\n",
    "* Build and evaluate flexible recommendation retrieval models.\n",
    "* Freely incorporate item, user, and context information into recommendation models.\n",
    "* Train multi-task models that jointly optimize multiple recommendation objectives.\n",
    "* TFRS is open source and available on <a href=\"https://github.com/tensorflow/recommenders\">Github</a>.\n",
    "\n",
    "To learn more, see the <a href = \"https://www.tensorflow.org/recommenders/examples/basic_retrieval'tutorial\"> on how to build a movie recommender system</a>, or check the API docs for the <a href= \"https://www.tensorflow.org/recommenders/api_docs/python/tfrs\">API reference.</a>\n",
    "\n",
    "# 3. The Plan of Attack\n",
    "A retail recommender system will normally have 2 phases:\n",
    "* Retrieving - Which is responsible for selecting an initial set of candidates from all possible candidates.\n",
    "* Ranking - Which takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations.\n",
    "\n",
    "In this notebook we are gonna create a only the retrieving model, which are simplier but yet powerfull.\n",
    "\n",
    "The basic idea is that we have two models, one consists in the query model (customer model) and the other, the candidate model (article model).\n",
    "Then, we combine these two models in a new model, which will perform the loss calculation and optimize the weights using ADAM optimzer.\n",
    "\n",
    "<img src=\"https://lh4.googleusercontent.com/CmzEJysJCdipgp1ntCzgZ5pdowgieyB43ep6cU_0WRpsOhXQy4aDWTZxd2IhV0A210TWIP41BpvSXKrGh5sv5Mya30lh1vKHQDiZK3wobWgIt23hx7pCZ3Em8TXMGt1mukuiu-2b\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Instaling and importing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-07T15:24:06.847744Z",
     "iopub.status.busy": "2022-05-07T15:24:06.847410Z",
     "iopub.status.idle": "2022-05-07T15:25:59.360718Z",
     "shell.execute_reply": "2022-05-07T15:25:59.359245Z",
     "shell.execute_reply.started": "2022-05-07T15:24:06.847707Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Normalization\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Dataset\n",
    "\n",
    "* images - a folder of images corresponding to each article_id; images are placed in subfolders starting with the first three digits of the article_id; note, not all article_id values have a corresponding image.\n",
    "* articles.csv - detailed metadata for each article_id available for purchase\n",
    "* customers.csv - metadata for each customer_id in dataset\n",
    "* sample_submission.csv - a sample submission file in the correct format\n",
    "* transactions_train.csv - the training data, consisting of the purchases each customer for each date, as well as additional information. Duplicate rows correspond to multiple purchases of the same\n",
    "\n",
    "For the sake of this notebook we are gonna skip the images dataset, as it can be implemented further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles_dataset = pd.read_csv(\"Data/articles.csv\")\n",
    "customers_dataset = pd.read_csv(\"Data/customers.csv\")\n",
    "train_dataset = pd.read_csv(\"Data/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105542 entries, 0 to 105541\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   article_id                    105542 non-null  int64 \n",
      " 1   product_code                  105542 non-null  int64 \n",
      " 2   prod_name                     105542 non-null  object\n",
      " 3   product_type_no               105542 non-null  int64 \n",
      " 4   product_type_name             105542 non-null  object\n",
      " 5   product_group_name            105542 non-null  object\n",
      " 6   graphical_appearance_no       105542 non-null  int64 \n",
      " 7   graphical_appearance_name     105542 non-null  object\n",
      " 8   colour_group_code             105542 non-null  int64 \n",
      " 9   colour_group_name             105542 non-null  object\n",
      " 10  perceived_colour_value_id     105542 non-null  int64 \n",
      " 11  perceived_colour_value_name   105542 non-null  object\n",
      " 12  perceived_colour_master_id    105542 non-null  int64 \n",
      " 13  perceived_colour_master_name  105542 non-null  object\n",
      " 14  department_no                 105542 non-null  int64 \n",
      " 15  department_name               105542 non-null  object\n",
      " 16  index_code                    105542 non-null  object\n",
      " 17  index_name                    105542 non-null  object\n",
      " 18  index_group_no                105542 non-null  int64 \n",
      " 19  index_group_name              105542 non-null  object\n",
      " 20  section_no                    105542 non-null  int64 \n",
      " 21  section_name                  105542 non-null  object\n",
      " 22  garment_group_no              105542 non-null  int64 \n",
      " 23  garment_group_name            105542 non-null  object\n",
      " 24  detail_desc                   105126 non-null  object\n",
      "dtypes: int64(11), object(14)\n",
      "memory usage: 20.1+ MB\n"
     ]
    }
   ],
   "source": [
    "articles_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1371980 entries, 0 to 1371979\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   customer_id             1371980 non-null  object \n",
      " 1   FN                      476930 non-null   float64\n",
      " 2   Active                  464404 non-null   float64\n",
      " 3   club_member_status      1365918 non-null  object \n",
      " 4   fashion_news_frequency  1355971 non-null  object \n",
      " 5   age                     1356119 non-null  float64\n",
      " 6   postal_code             1371980 non-null  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 73.3+ MB\n"
     ]
    }
   ],
   "source": [
    "customers_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   t_dat             object \n",
      " 1   customer_id       object \n",
      " 2   article_id        int64  \n",
      " 3   price             float64\n",
      " 4   sales_channel_id  int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Memory\n",
    "Once the dataset is huge and our model will have some complexity, one of the most important things to do, is to use the memory in the most efficient way possible.\n",
    "\n",
    "The customer_id is a length 64 string which uses 64 bytes. \n",
    "\n",
    "The code above coverts the column to int64 which only takes 8 bytes!\n",
    "\n",
    "We assert that the article_id is converted to int32 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving Memory\n",
    "customers_dataset[\"customer_id\"] = customers_dataset.customer_id.apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "articles_dataset[\"article_id\"] = articles_dataset[\"article_id\"].astype(np.int32)\n",
    "train_dataset[\"customer_id\"] = train_dataset.customer_id.apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "train_dataset[\"article_id\"] = train_dataset[\"article_id\"].astype(np.int32)\n",
    "\n",
    "# Preprocessing\n",
    "age = customers_dataset.age.values\n",
    "age = MinMaxScaler().fit_transform(age.reshape(-1,1)).T[0]\n",
    "customers_dataset[\"age\"] = age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving as Parquet\n",
    "Now, to achieve a better performance when reading the datasets, we save then as parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Parquet\n",
    "articles_dataset.to_parquet(\"articles.parquet.gzip\", compression='gzip')\n",
    "customers_dataset.to_parquet(\"customers.parquet.gzip\", compression='gzip')\n",
    "train_dataset.to_parquet(\"transactions.parquet.gzip\", compression='gzip')\n",
    "\n",
    "# Del the used variables so far to reduce the memory usage\n",
    "del articles_dataset\n",
    "del customers_dataset\n",
    "del train_dataset\n",
    "del age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Parquet\n",
    "articles_dataset = pd.read_parquet(\"Reduced_Data/articles.parquet.gzip\")\n",
    "customers_dataset = pd.read_parquet(\"Reduced_Data/customers.parquet.gzip\")\n",
    "train_dataset = pd.read_parquet(\"Reduced_Data/transactions.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train/Test\n",
    "train_dataset = train_dataset[train_dataset.t_dat >= \"2020-09-01\"]\n",
    "#test_dataset = train_dataset[train_dataset.t_dat >= \"2020-09-19\"]\n",
    "#train_dataset = train_dataset[train_dataset.t_dat < \"2020-09-19\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will be very useful further in merging the features in one dataset into another with less use of memory.\n",
    "\"\"\"\n",
    "def merger(left,right,var,on):  \n",
    "    mapper = right[[on,var]].set_index(on).to_dict()[var]\n",
    "    left[var] = left[on].map(mapper)\n",
    "    return left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Articles Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Articles\n",
    "articles_dataset = merger(articles_dataset,train_dataset,\"price\",\"article_id\").fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Customers Dataset\n",
    "We have already parsed the age feature, so we can skip this, but if you want to make some feature engineering with the customers dataset, this session can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Training Dataset\n",
    "Now we can merge all features we have into the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "train_dataset = merger(train_dataset,customers_dataset,\"age\",\"customer_id\").fillna(-1)\n",
    "#test_dataset = merger(test_dataset,customers_dataset,\"age\",\"customer_id\").fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Converting to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tensors\n",
    "candidates_tensor = tf.data.Dataset.from_tensor_slices(dict(articles_dataset[[\"article_id\",\"price\"]]))\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices(dict(train_dataset[['customer_id','article_id','age','price']])).shuffle(100000).batch(5000).cache()\n",
    "#test_tensor = tf.data.Dataset.from_tensor_slices(dict(test_dataset[['customer_id','article_id','age','price']])).shuffle(100000).batch(5000).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Getting Uniques\n",
    "Here we get the vocabularies of customer_id and article_id to be inserted into the IntegerLookup layer of our models.\n",
    "\n",
    "This is important because we must have a integer index in our embedding layer.\n",
    "\n",
    "We get their lenghts as well to give to the embedding layer its input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting uniques\n",
    "unique_customers_ids = customers_dataset.customer_id.unique()\n",
    "unique_articles_ids = articles_dataset.article_id.unique()\n",
    "\n",
    "unique_customers = len(unique_customers_ids)\n",
    "unique_articles = len(unique_articles_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can del our used variables to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del articles_dataset\n",
    "del customers_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Creating the Model\n",
    "## 7.1 Customer/Query Model\n",
    "\n",
    "Lets start by creating the Customer/Query model.\n",
    "\n",
    "The first step is to create a embedding model (customer_id_model).\n",
    "\n",
    "One of the biggest advantages of using the TFRS is the facility to input context features into the model, in our case we are gonna input the age feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.customer_id_model = tf.keras.Sequential()\n",
    "        self.customer_id_model.add(IntegerLookup(vocabulary = unique_customers_ids,mask_token = None))\n",
    "        self.customer_id_model.add(Embedding(unique_customers + 1,200))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        reshaped_age = tf.reshape(inputs['age'],(-1,1))\n",
    "        return tf.concat([self.customer_id_model(inputs[\"customer_id\"]),\n",
    "                          reshaped_age],axis=1)\n",
    "    \n",
    "class QueryModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = CustomerModel()\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(100))\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        feature_embeddings = self.embedding_layer(inputs)\n",
    "        return self.dense_layers(feature_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Article/Candidate Model\n",
    "\n",
    "Here we create the Article/Candidate Model. \n",
    "\n",
    "First, we need to create the embedding model, and afterwards we create the candidate model, inputing the price feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.article_id_model = tf.keras.Sequential()\n",
    "        self.article_id_model.add(IntegerLookup(vocabulary = unique_articles_ids,mask_token = None))\n",
    "        self.article_id_model.add(Embedding(unique_articles + 1, 200))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        reshaped_price = tf.reshape(inputs[\"price\"],(-1,1))\n",
    "        return tf.concat([self.article_id_model(inputs[\"article_id\"]),\n",
    "                          reshaped_price],axis=1)\n",
    "    \n",
    "class CandidateModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = ArticleModel()\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(100))\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        feature_embeddings = self.embedding_layer(inputs)\n",
    "        return self.dense_layers(feature_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Combined Model\n",
    "\n",
    "Then we combine the two models into a new one, the Combined Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel()\n",
    "        self.candidate_model = CandidateModel()\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics = tfrs.metrics.FactorizedTopK(\n",
    "                candidates=candidates_tensor.batch(128).map(self.candidate_model)))\n",
    "        \n",
    "    def compute_loss(self, features, training = False):\n",
    "        query_dict = {\"customer_id\":features[\"customer_id\"],\"age\":features[\"age\"]}\n",
    "        query_outcome = self.query_model(query_dict)\n",
    "        \n",
    "        candidate_dict = {\"article_id\":features[\"article_id\"],\"price\":features[\"price\"]}\n",
    "        candidate_outcome = self.candidate_model(candidate_dict)\n",
    "        \n",
    "        return self.task(query_outcome, candidate_outcome, compute_metrics = not training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Training the Model\n",
    "\n",
    "Before we train our model, we are going to declare some callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombinedModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#csv_logger_callback = tf.keras.callbacks.CSVLogger(\"logger.csv\", separator=',', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 38s 277ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42571.8490 - regularization_loss: 0.0000e+00 - total_loss: 42571.8490\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 36s 274ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42485.2059 - regularization_loss: 0.0000e+00 - total_loss: 42485.2059\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 37s 277ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 42251.3384 - regularization_loss: 0.0000e+00 - total_loss: 42251.3384\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 37s 279ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 41787.8874 - regularization_loss: 0.0000e+00 - total_loss: 41787.8874\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 36s 270ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 41390.2111 - regularization_loss: 0.0000e+00 - total_loss: 41390.2111\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 37s 276ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 41125.9137 - regularization_loss: 0.0000e+00 - total_loss: 41125.9137\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 36s 274ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 40908.2512 - regularization_loss: 0.0000e+00 - total_loss: 40908.2512\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 37s 278ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 40718.6382 - regularization_loss: 0.0000e+00 - total_loss: 40718.6382\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 36s 267ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 40566.9988 - regularization_loss: 0.0000e+00 - total_loss: 40566.9988\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 34s 259ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 40445.0500 - regularization_loss: 0.0000e+00 - total_loss: 40445.0500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_tensor,    \n",
    "    epochs=100,\n",
    "    verbose=1, #callbacks = [csv_logger_callback],\n",
    "    #validation_data=test_tensor,\n",
    "    #validation_freq=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scanning and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-07T02:57:25.758079Z",
     "iopub.status.busy": "2022-05-07T02:57:25.757767Z",
     "iopub.status.idle": "2022-05-07T02:57:36.574415Z",
     "shell.execute_reply": "2022-05-07T02:57:36.57348Z",
     "shell.execute_reply.started": "2022-05-07T02:57:25.758048Z"
    }
   },
   "outputs": [],
   "source": [
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.query_model,k=12)\n",
    "scann_index.index_from_dataset(tf.data.Dataset.zip((candidates_tensor.map(lambda x : x[\"article_id\"]).batch(100), candidates_tensor.batch(100).map(model.candidate_model))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Submiting the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-07T02:57:39.989809Z",
     "iopub.status.busy": "2022-05-07T02:57:39.989493Z",
     "iopub.status.idle": "2022-05-07T02:57:44.923541Z",
     "shell.execute_reply": "2022-05-07T02:57:44.922734Z",
     "shell.execute_reply.started": "2022-05-07T02:57:39.989763Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-07T03:01:11.781736Z",
     "iopub.status.busy": "2022-05-07T03:01:11.780942Z",
     "iopub.status.idle": "2022-05-07T03:01:16.409686Z",
     "shell.execute_reply": "2022-05-07T03:01:16.408979Z",
     "shell.execute_reply.started": "2022-05-07T03:01:11.781684Z"
    }
   },
   "outputs": [],
   "source": [
    "#Converter Customer Id\n",
    "sub['customer_id_int64'] = sub.customer_id.apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "customer_id_dict = sub[[\"customer_id\",\"customer_id_int64\"]].set_index(\"customer_id_int64\").to_dict()\n",
    "sub[\"customer_id\"] = sub[\"customer_id_int64\"]\n",
    "\n",
    "# Merge Age and Fillna\n",
    "customers_dataset = pd.read_parquet(\"./customers.parquet.gzip\")\n",
    "sub_train = merger(sub,customers_dataset,\"age\",\"customer_id\").fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-07T03:31:52.602024Z",
     "iopub.status.busy": "2022-05-07T03:31:52.601398Z"
    }
   },
   "outputs": [],
   "source": [
    "_,articles = scann_index(dict(sub_train[['customer_id','age']]))\n",
    "predictions = np.array(articles).astype(str)\n",
    "mapper = np.vectorize(lambda x : x.zfill(10))\n",
    "zfilled_preds = [mapper(row) for row in pred_array]\n",
    "predictions = pd.Series(map(' '.join, predictions))\n",
    "sub['prediction'] = predictions\n",
    "sub[\"customer_id\"] = sub['customer_id'].map(customer_id_dict[\"customer_id\"])\n",
    "sub.to_csv(\"submission.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
